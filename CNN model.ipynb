{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "relu = nn.functional.relu\n",
    "softmax = nn.functional.softmax\n",
    "from torch.nn.functional import cross_entropy\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "from os import listdir\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1730c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    # initializer function\n",
    "    def __init__(self, input_shape=(1, 199,336), classes=9):\n",
    "        super(Model, self).__init__()\n",
    "        # five convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=8, kernel_size=3, stride=1)       \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1)\n",
    "        # one dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # one dense layer\n",
    "        self.final_dense = nn.Linear(11200, classes) \n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm5= nn.BatchNorm2d(128)\n",
    "\n",
    "        for m in (self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.final_dense):   #converts from default weight normalization to glorot(xavier)\n",
    "            nn.init.constant_(m.bias,0)\n",
    "            nn.init.xavier_normal_(m.weight, np.sqrt(2))\n",
    "\n",
    "    # forward-pass function\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))     #after each convolution, pool and normalize\n",
    "        x = self.pool(x)            #used to reduce dimensionality\n",
    "        x = self.batchnorm1(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        \n",
    "#         x = relu(self.conv4(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm4(x)\n",
    "        \n",
    "#         x = relu(self.conv5(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm5(x)\n",
    "        \n",
    "        x = x.reshape(len(x),-1)\n",
    "        x = self.dropout(x)\n",
    "        x = softmax(self.final_dense(x), dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d65ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truthdata):\n",
    "    #Returns the mean classification accuracy for a batch of predictions.\n",
    "    ''''\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        The scores for D classes, for a batch of M data points\n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, D)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    '''''\n",
    "    return np.mean(np.argmax(predictions, axis=1) == truth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelslist = range(len(img_data))\n",
    "onehotslist = []\n",
    "for label in labelslist:\n",
    "    blank = np.zeros(len(labelslist))\n",
    "    blank[label] = 1\n",
    "    onehotslist.append(blank)\n",
    "\n",
    "images_dir = Path(r\"C:\\Users\\g_bab\\Downloads\\audiosamples\").expanduser()\n",
    "labelslist = []\n",
    "x_data=[]\n",
    "xtest_data = []\n",
    "labelslisttest = []\n",
    "for fname in listdir(images_dir):\n",
    "    count = 0\n",
    "    counttest = 0\n",
    "    iteration = 0 \n",
    "    print(fname)\n",
    "    for second in listdir(r\"C:\\Users\\g_bab\\Downloads\\audiosamples\\{}\".format(fname)):\n",
    "        fpath = os.path.join(images_dir, fname)\n",
    "        finalpath = os.path.join(fpath, second)\n",
    "        #print(finalpath)\n",
    "        y, sr = librosa.load(finalpath, duration=5)\n",
    "        im = librosa.feature.melspectrogram(y=y, sr=sr)  \n",
    "        #print(im.shape)\n",
    "        #im = np.array(im, dtype = np.float32)\n",
    "        if im.shape == (128,216):\n",
    "            if iteration%4 == 0:\n",
    "                xtest_data.append(im)\n",
    "                counttest += 1\n",
    "            else:\n",
    "                x_data.append(im) \n",
    "                count += 1\n",
    "        iteration+=1\n",
    "        #print(fname, type(fname))\n",
    "    for i in range(count):\n",
    "        labelslist.append(one_hots[fname])\n",
    "    for i in range(counttest):\n",
    "        labelslisttest.append(one_hots[fname])\n",
    "        \n",
    "    \n",
    "\n",
    "finaldata = []\n",
    "for arr in x_data:\n",
    "    arr = arr[np.newaxis,...]\n",
    "    arr = arr/255 #scaling it down\n",
    "    finaldata.append(arr)\n",
    "x_data = finaldata\n",
    "\n",
    "\n",
    "finaltestdata = []\n",
    "for arr in xtest_data:\n",
    "    arr = arr[np.newaxis,...]\n",
    "    arr = arr/255 #scaling it down\n",
    "    finaltestdata.append(arr)\n",
    "xtest_data = finaltestdata\n",
    "\n",
    "\n",
    "x_train = x_data       \n",
    "x_test = xtest_data\n",
    "\n",
    "\n",
    "y_train = labelslist\n",
    "y_test = labelslisttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SETUP\n",
    "\n",
    "model = Model()#.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), weight_decay=14e-3)\n",
    "batch_size = 100 #should be 100\n",
    "epochs = 70                      #recommended 70\n",
    "\n",
    "#print(x_train[:3])\n",
    "\n",
    "for epoch_cnt in range(epochs):\n",
    "    idxs = np.arange(len(x_train))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)  \n",
    "\n",
    "    for batch_cnt in range(len(x_train)//batch_size):\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "        batch = [x_train[index] for index in batch_indices]  # random batch of our training data\n",
    "        #np.stack(batch)\n",
    "        #print(batch[:3])\n",
    "        optim.zero_grad()  #pytorch accumulates gradients\n",
    "        spec = np.stack(batch)\n",
    "        #print(spec.shape)\n",
    "        spec = torch.tensor(spec)\n",
    "        #spec = spec.double()\n",
    "        \n",
    "        #print(type(spec))\n",
    "        # compute the predictions for this batch by calling on model\n",
    "        \n",
    "        prediction = model(spec)\n",
    "        #print(prediction)\n",
    "        #print(torch.sum(prediction))\n",
    "        # compute the true (a.k.a desired) values for this batch: \n",
    "        truth = [y_train[indexes] for indexes in batch_indices]\n",
    "        truth = torch.tensor(truth).argmax(dim = 1)\n",
    "#         if any(tru>3 for tru in truth):\n",
    "#             print(truth)\n",
    "        # compute the loss associated with our predictions(use softmax_cross_entropy)\n",
    "        loss = cross_entropy(prediction, truth) \n",
    "        \n",
    "        #print(type(truth), type(prediction))\n",
    "        # back-propagate through your computational graph through your loss\n",
    "        loss.backward()\n",
    "\n",
    "        # execute gradient descent by calling step() of optim\n",
    "        optim.step()\n",
    "\n",
    "        # compute the accuracy between the prediction and the truth \n",
    "        truth = truth.detach().numpy()\n",
    "        prediction = prediction.detach().numpy()\n",
    "        acc = accuracy(prediction, truth)\n",
    "\n",
    "        # set the training loss and accuracy\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "    # Here, we evaluate our model on batches of *testing* data\n",
    "    # this will show us how good our model does on data that \n",
    "    # it has never encountered\n",
    "    # Iterate over batches of *testing* data\n",
    "    for batch_cnt in range(0, len(x_test)//batch_size):\n",
    "        idxs = np.arange(len(x_test))\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "        batch = [x_test[index] for index in batch_indices]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get your model's prediction on the test-batch\n",
    "            spec = np.stack(batch)\n",
    "            spec = torch.tensor(spec)\n",
    "            \n",
    "            truth = [y_test[indexy] for indexy in batch_indices]\n",
    "            truth = np.array(truth)\n",
    "            truth = np.argmax(truth, axis = 1)\n",
    "            #truth = truth.detach().numpy()\n",
    "            \n",
    "            prediction = model(spec)\n",
    "            np.stack(prediction)\n",
    "            prediction = prediction.detach().numpy()\n",
    "            \n",
    "            # get the truth values for that test-batch\n",
    "            \n",
    "\n",
    "            # compute the test accuracy\n",
    "            acc = accuracy(prediction, truth)\n",
    "\n",
    "        # log the test-accuracy in noggin\n",
    "        plotter.set_test_batch({\"accuracy\": acc}, batch_size=batch_size)\n",
    "\n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()\n",
    "plotter.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
